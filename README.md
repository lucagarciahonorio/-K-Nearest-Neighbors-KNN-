# K-Nearest Neighbors (KNN) Comparison Project

## ğŸ“Œ Overview
This project compares two implementations of the **K-Nearest Neighbors (KNN) algorithm**:
1. **Using Scikit-Learn**: Optimized and efficient implementation.
2. **Manual Implementation**: KNN from scratch using NumPy.

The project evaluates performance based on:
- Execution time â³
- Accuracy ğŸ”¢
- Precision, Recall, and F1-score ğŸ“Š
- Confusion Matrix ğŸŸ¦
- Decision Boundaries ğŸ¨

## ğŸ“‚ Project Structure
```
K-Nearest-Neighbors-KNN/
â”‚â”€â”€ scrtips  # folder with files
    â”‚â”€â”€ KNN.py   # KNN using Scikit-Learn and Manual KNN implementation
â”‚â”€â”€ README.md       # Documentation
```

## ğŸ› ï¸ Installation
Ensure you have Python installed (>=3.8). Install the required libraries:
```bash
pip install numpy pandas scikit-learn matplotlib seaborn
```

## ğŸš€ How to Run
1ï¸âƒ£ Run the **Scikit-Learn KNN** script:
```bash
python knn_sklearn.py
```
2ï¸âƒ£ Run the **Manual KNN** script:
```bash
python knn_manual.py
```

## ğŸ“Š Expected Results
Both implementations should achieve similar accuracy (~95%), but Sklearn KNN will be significantly faster.

| Model           | Execution Time (sec) | Accuracy |
|----------------|---------------------|----------|
| **Sklearn KNN** | **Fast âš¡ (~0.001s)** | **High âœ… (~95%)** |
| **Manual KNN**  | **Slower ğŸ¢ (~0.1-1s)** | **Similar âœ… (~95%)** |

## ğŸ“ˆ Visualization
- **Confusion Matrix** (Shows classification performance)
- **Decision Boundary Plot** (Shows how data is classified)

## ğŸ” Next Steps
- Optimize manual KNN using NumPy vectorization.
- Test with larger datasets like **MNIST** or **Titanic**.
- Tune `k` value using `GridSearchCV`.

---
ğŸ’¡ **Author:** Luca Garcia  
ğŸ“… **Date:** 2025  
ğŸš€ **Enjoy coding KNN!**

